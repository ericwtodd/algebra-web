<!doctype html>
<html lang="en">

<head>
    <title>In-Context Algebra</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="description"
        content="" />
    <meta property="og:title" content="In-Context Algebra" />
    <meta property="og:url" content="https://algebra.baulab.info/" />
    <meta property="og:image" content="https://algebra.baulab.info/images/algebra-thumb.png" />
    <meta property="og:description" content="Transformers use symbolic algorithms to solve math problems when tokens are variables whose meaning can only be inferred in-context.">
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="In-Context Algebra" />
    <meta name="twitter:description"
        content="Understanding the learned algorithms of transformer language models solving abstract algebra problems through in-context learning." />
    <meta name="twitter:image" content="https://algebra.baulab.info/images/algebra-thumb.png" />
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <style>
        .relatedthumb {
            float: left;
            width: 200px;
            margin: 3px 10px 7px 0;
        }

        .relatedblock {
            clear: both;
            display: inline-block;
        }

        .bold-sc {
            font-variant: small-caps;
            font-weight: bold;
        }

        .cite,
        .citegroup {
            margin-bottom: 8px;
        }

        :target {
            background-color: yellow;
        }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
    </script>

</head>

<body class="nd-docs">
    <div class="nd-pageheader">
        <div class="container">
            <h1 class="lead">
                <nobr class="widenobr">In-Context Algebra</nobr>
            </h1>
            <address>
                <nobr><a href="https://ericwtodd.github.io/" target="_blank">Eric Todd</a><sup>1</sup>,</nobr>
                <nobr><a href="https://www.jannikbrinkmann.com/" target="_blank">Jannik Brinkmann</a><sup>1,2</sup>,</nobr>
                <nobr><a href="https://rohitgandikota.github.io/" target="_blank">Rohit Gandikota</a><sup>1</sup>,</nobr>
                <nobr><a href="https://baulab.info/" target="_blank">David Bau</a><sup>1</sup></nobr><br>
                <nobr><sup>1</sup><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern University</a>,</nobr>
                <nobr><sup>2</sup><a href="https://www.tu-clausthal.de/en/" target="_blank">Technical University Clausthal</a></nobr>
            </address>
        </div>
    </div><!-- end nd-pageheader -->

    <div class="container">
        <div class="row justify-content-center" style="margin-bottom: 20px">
        </div>
        <div class="row justify-content-center text-center">

            <p>
                <a href="" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="ArXiv Preprint thumbnail" data-nothumb="">
                    <br>ArXiv<br>Preprint</a>
                <a href="https://github.com/ericwtodd/algebra" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="Github code thumbnail" data-nothumb="">
                    <br>Source Code<br>Github
                </a>
                <a href="" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/data-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="Dataset thumbnail" data-nothumb="">
                    <br>Data<br>
                </a>

            </p>

            <div class="card" style="max-width: 1020px;">
                <div class="card-block">
                <h3>What Do Transformers Learn Without Meaningful Token Embeddings?</h3>
                <p>
                    Much of the performance of language models (LMs) can be attributed to the power of token embeddings;
                    prior work has shown that token embeddings can pre-encode rich semantic, syntactic, and numeric structure.
                    However, the hallmark of <b>abstract reasoning</b> is the ability to work with words and symbols whose meaning is unknown ahead of time.
                </p>
                <p>
                    In this paper, we design an in-context learning setting to study the computational strategies that transformers develop to solve abstract arithmetic tasks.
                    What makes our setting unique is that <b>each token is a variable</b> that can represent any algebraic element, and <b>tokens acquire meaning only through their interactions within a sequence.</b>
                    <!-- The meaning of each token is only fixed within a single sequence. -->
                    In contrast to the geometric representations learned for numeric reasoning seen in prior work (where token meanings are fixed), we find that models develop symbolic reasoning mechanisms based on sparse relational patterns. 
                    This suggests that the kind of reasoning strategies learned by transformers are dependent on the task structure.
                    We identify three primary algorithmic strategies models consistently employ beyond verbatim copying to solve the in-context algebra task: commutative copying, identity element recognition, and closure-based cancellation.                                      
                </p>
                </div><!--card-block-->
                </div><!--card-->

        </div><!--row-->

        <div class="row">
            <div class="col">

                <h2>The Task: In-Context Algebra</h2>
                

                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/paper/data_assign_generate.gif" class="smallfig" style="width:100%">
                    <figcaption>
                        Data Generation Process
                    </figcaption>
                </figure>

                <h2>HEADER</h2>

                <h2>Hypothesizing Model Mechanisms</h2>

                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/paper/coverage.png" style="width:100%">
                    <figcaption>
                        Algorithmic Coverage
                    </figcaption>
                </figure>

                <h2>Verifying Mechanisms</h2>

                <h3>Copying and Commutative Copying</h3>

                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/paper/" style="width:100%">
                    <figcaption>
                        Copying
                    </figcaption>
                </figure>

                <h3>Identity Element Recognition</h3>

                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/paper/" style="width:100%">
                    <figcaption>
                        Identity Element Recognition
                    </figcaption>
                </figure>

                <h3>Closure-Based Cancellation</h3>

                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/paper/" style="width:100%">
                    <figcaption>
                        Closure-Based Cancellation
                    </figcaption>
                </figure>


                <h2>Phase Transitions</h2>

                <figure class="center_image" style="margin-top: 30px">
                    <img src="images/paper/phase_transition.png" style="width:100%">
                    <figcaption>
                        Phase Transitions
                    </figcaption>
                </figure>

                
                
            

                
                <!-- <h2>Related Work</h2> -->

                <h2 >Related Work</h2>


                
                <!-- <p class="citation"><a href=""><img src="images/related_work/" alt=""></a><br>
                    <b>Notes:</b> 
                </p>  -->

                <h2>Strategies for Numeric Reasoning in Fixed-Symbol Arithmetic</h2>
                

                <p class="citation"><a href="https://arxiv.org/pdf/2201.02177"><img src="images/related_work/power-grokking.png" alt="power-grokking-2022">Althea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. 2022.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2205.10343"><img src="images/related_work/liu-grokking.png" alt="liu-grokking-2022">Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric Michaud, Max Tegmark, Mike Williams. Towards Understanding Grokking: An Effective Theory of Representation Learning. 2022.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2410.21272"><img src="images/related_work/nikankin-arithmetic.png" alt="nikankin-arithmetic-2025">Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov. Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2406.03445"><img src="images/related_work/zhou-fourier.png" alt="zhou-fourier-2024">Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia. Pre-trained Large Language Models Use Fourier Features to Compute Addition. 2024.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2301.05217"><img src="images/related_work/nanda-grokking.png" alt="nanda-grokking-2023">Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt. Progress Measures for Grokking via Mechanistic Interpretability. 2023.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2306.17844"><img src="images/related_work/zhong-pizza-clock.png" alt="zhong-clock-2023">Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas. The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. 2023.</a><br>
                    <b>Notes:</b> 
                </p>

                <p class="citation"><a href="https://arxiv.org/pdf/2410.04368"><img src="images/related_work/zhong-random.png" alt="zhong-random-2024">Ziqian Zhong, Jacob Andreas. Algorithmic Capabilities of Random Transformers. 2024.</a><br>
                    <b>Notes:</b> 
                </p>

                <p class="citation"><a href="https://arxiv.org/pdf/2406.02550"><img src="images/related_work/he-double-grokking.png" alt="">Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov. Learning to Grok: Emergence of In-Context Learning and Skill Composition in Modular Arithmetic Tasks</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2510.00184"><img src="images/related_work/bai-multiplication.png" alt="">Xiaoyan Bai, Itamar Pres, Yuntian Deng, Chenhao Tan, Stuart Shieber, Fernanda Viegas, Martin Wattenberg, Andrew Lee. Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls. 2025.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2505.05145"><img src="images/related_work/hu-addition-subspace.png" alt="">Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen. Understanding In-Context Learning of Addition via Activation Subspaces. 2025.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2410.05229"><img src="images/related_work/mirzadeh-gsm_symbolic.png" alt="">Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. 2025.</a><br>
                    <b>Notes:</b> 
                </p> 

                <p class="citation"><a href="https://arxiv.org/pdf/2502.00873"><img src="images/related_work/kantamneni-addition.png" alt="kantamneni-addition-2025"> Subhash Kantamneni, Max Tegmark. Language Models Use Trigonometry to do Addition. 2025.</a><br>
                    <b>Notes:</b> 
                </p> 

                

	
                <h2>How to cite</h2>

                <p>The paper can be cited as follows.
                </p>

                <div class="card">
                    <h3 class="card-header">bibliography</h3>
                    <div class="card-block">
                        <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
                        Eric Todd, Jannik Brinkmann, Rohit Gandikota, and David Bau. "<em>In-Context Algebra.</em>" arXiv preprint arXiv:2512.XXXXX, (2025).
                        </p>
                    </div>
                    <h3 class="card-header">bibtex</h3>
                    <div class="card-block">
                        <pre class="card-text clickselect">
@article{todd2025incontextalgebra,
    title={In-Context Algebra}, 
    author={Eric Todd and Jannik Brinkmann and Rohit Gandikota and David Bau},
    journal={arxiv preprint arXiv:2512.XXXXX},
    year={2025},
    url={https://arxiv.org/abs/2512.XXXXX}
}</pre>
                    </div>
                </div>
                <!-- </p> -->

            <!-- </div> -->
            </div> <!--col -->    
        </div> <!--row -->
    </div> <!-- container -->

    

    <footer class="nd-pagefooter">
        <div class="row">
            <div class="col-6 col-md text-center">
                <a href="https://baulab.info/">About the Bau Lab</a>
            </div>
        </div>
    </footer>

</body>
<script>
    $(document).on('click', '.clickselect', function (ev) {
        var range = document.createRange();
        range.selectNodeContents(this);
        var sel = window.getSelection();
        sel.removeAllRanges();
        sel.addRange(range);
    });
</script>

</html>
